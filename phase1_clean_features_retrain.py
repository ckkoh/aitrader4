#!/usr/bin/env python3
"""
PHASE 1, DAY 1: Remove Data Leakage & Retrain with Clean Features

Critical Improvements:
1. Remove data leakage features (target_regression, future_return, target_binary)
2. Select top 20 clean features by importance
3. Retrain Random Forest with clean features
4. Validate improvement
"""

import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import json

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from feature_engineering import FeatureEngineering

print("=" * 80)
print("PHASE 1: CLEAN FEATURES & RETRAIN")
print("=" * 80)

# ============================================================================
# STEP 1: Load Data
# ============================================================================
print("\n[1/6] Loading Data...")

df = pd.read_csv('sp500_historical_data.csv', index_col='Date', parse_dates=True)
df = df.sort_index()

train_df = df[(df.index >= '2020-01-01') & (df.index < '2025-01-01')]
val_df = df[df.index >= '2025-01-01']

print(f"  Train: {len(train_df)} rows (2020-2024)")
print(f"  Val:   {len(val_df)} rows (2025)")

# ============================================================================
# STEP 2: Generate Features & Identify Leakage
# ============================================================================
print("\n[2/6] Generating Features & Identifying Leakage...")

train_feat = FeatureEngineering.build_complete_feature_set(train_df, include_volume=True).dropna()
val_feat = FeatureEngineering.build_complete_feature_set(val_df, include_volume=True).dropna()

# Create target
train_target = (train_feat['close'].pct_change().shift(-1) > 0).astype(int)[:-1]
val_target = (val_feat['close'].pct_change().shift(-1) > 0).astype(int)[:-1]

train_feat = train_feat.iloc[:-1]
val_feat = val_feat.iloc[:-1]

# CRITICAL: Identify data leakage features
leakage_features = [
    'target_regression',   # This IS the target!
    'future_return',       # This is future data!
    'target_binary',       # This IS the target!
    'target_class',        # This IS the target!
    'future_return_5',     # Future data
    'future_return_10',    # Future data
    'future_volatility',   # Future data
]

# Also exclude OHLCV
exclude_features = ['open', 'high', 'low', 'close', 'volume'] + leakage_features

# Get all feature names
all_features = [c for c in train_feat.columns if c not in exclude_features]

print(f"\n  Total features generated: {len(train_feat.columns)}")
print(f"  LEAKAGE features removed: {len(leakage_features)}")
print(f"  Clean features available: {len(all_features)}")

# ============================================================================
# STEP 3: Train Temporary Model to Get Feature Importance
# ============================================================================
print("\n[3/6] Training Temporary Model to Rank Features...")

X_train_all = train_feat[all_features].replace([np.inf, -np.inf], np.nan).fillna(0)
y_train = train_target
X_val_all = val_feat[all_features].replace([np.inf, -np.inf], np.nan).fillna(0)
y_val = val_target

# Train with all clean features
temp_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_leaf=20,
    random_state=42,
    n_jobs=-1
)
temp_model.fit(X_train_all, y_train)

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': all_features,
    'importance': temp_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n  Top 20 Most Important Clean Features:")
for i, row in feature_importance.head(20).iterrows():
    print(f"    {i + 1:2d}. {row['feature']:35s} {row['importance']:.4f}")

# ============================================================================
# STEP 4: Select Top 20 Clean Features
# ============================================================================
print("\n[4/6] Selecting Top 20 Clean Features...")

top_20_features = feature_importance.head(20)['feature'].tolist()

X_train_clean = X_train_all[top_20_features]
X_val_clean = X_val_all[top_20_features]

print(f"  Selected features: {len(top_20_features)}")

# ============================================================================
# STEP 5: Train Final Model with Clean Features
# ============================================================================
print("\n[5/6] Training Final Model with Clean Features...")

clean_model = RandomForestClassifier(
    n_estimators=200,      # More trees
    max_depth=10,
    min_samples_leaf=20,
    random_state=42,
    n_jobs=-1
)

clean_model.fit(X_train_clean, y_train)

# Evaluate
y_train_pred = clean_model.predict(X_train_clean)
y_val_pred = clean_model.predict(X_val_clean)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
val_prec = precision_score(y_val, y_val_pred)
val_rec = recall_score(y_val, y_val_pred)
val_f1 = f1_score(y_val, y_val_pred)

print("\n  Performance with CLEAN Features:")
print(f"    Train Accuracy: {train_acc:.3f}")
print(f"    Val Accuracy:   {val_acc:.3f}")
print(f"    Val Precision:  {val_prec:.3f}")
print(f"    Val Recall:     {val_rec:.3f}")
print(f"    Val F1:         {val_f1:.3f}")

# ============================================================================
# STEP 6: Compare with Original Model
# ============================================================================
print("\n[6/6] Comparing with Original Model...")

# Load original model
original_model_file = 'models/randomforest_2020-2024train_2025val_20251230_210110.pkl'
with open(original_model_file, 'rb') as f:
    original_data = pickle.load(f)
    original_model = original_data['model']
    original_features = original_data['features']

# Evaluate original model
X_val_original = val_feat[original_features].replace([np.inf, -np.inf], np.nan).fillna(0)
y_val_pred_original = original_model.predict(X_val_original)
original_val_acc = accuracy_score(y_val, y_val_pred_original)

print(f"\n{'=' * 80}")
print("COMPARISON: Original vs Clean")
print(f"{'=' * 80}")
print(f"{'Metric':<25} {'Original (86 feat)':<20} {'Clean (20 feat)':<20} {'Improvement'}")
print(f"{'-' * 80}")
print(
    f"{
        'Features Used':<25} {
            len(original_features):<20} {
                len(top_20_features):<20} {
                    len(top_20_features) -
        len(original_features)}")
print(f"{'Val Accuracy':<25} {original_val_acc:.3f}{'':<16} {val_acc:.3f}{'':<16} {(val_acc - original_val_acc):+.3f}")
print(f"{'Val Precision':<25} {'N/A':<20} {val_prec:.3f}{'':<16}")
print(f"{'Val Recall':<25} {'N/A':<20} {val_rec:.3f}{'':<16}")
print(f"{'Val F1':<25} {'N/A':<20} {val_f1:.3f}{'':<16}")
print(f"{'Had Data Leakage?':<25} {'YES (3 features)':<20} {'NO':<20}")

# ============================================================================
# STEP 7: Save Clean Model
# ============================================================================
print("\nðŸ’¾ Saving Clean Model...")

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
clean_model_file = f'models/randomforest_CLEAN_top20_{timestamp}.pkl'

with open(clean_model_file, 'wb') as f:
    pickle.dump({
        'model': clean_model,
        'features': top_20_features,
        'feature_importance': feature_importance.head(20).to_dict('records'),
        'performance': {
            'train_accuracy': float(train_acc),
            'val_accuracy': float(val_acc),
            'val_precision': float(val_prec),
            'val_recall': float(val_rec),
            'val_f1': float(val_f1)
        },
        'training_date': timestamp,
        'notes': 'Clean model - no data leakage, top 20 features only'
    }, f)

print(f"  âœ“ Saved: {clean_model_file}")

# Save comparison report
report = {
    'timestamp': datetime.now().isoformat(),
    'phase': 'Phase 1 - Data Leakage Removal',
    'original_model': {
        'features': len(original_features),
        'val_accuracy': float(original_val_acc),
        'data_leakage': True,
        'leakage_features': leakage_features
    },
    'clean_model': {
        'features': len(top_20_features),
        'val_accuracy': float(val_acc),
        'val_precision': float(val_prec),
        'val_recall': float(val_rec),
        'val_f1': float(val_f1),
        'data_leakage': False,
        'selected_features': top_20_features
    },
    'improvement': {
        'accuracy_change': float(val_acc - original_val_acc),
        'features_reduced': len(original_features) - len(top_20_features),
        'data_quality': 'CLEAN'
    }
}

report_file = 'models/phase1_clean_features_report.json'
with open(report_file, 'w') as f:
    json.dump(report, f, indent=2)

print(f"  âœ“ Report saved: {report_file}")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("PHASE 1 STEP 1 COMPLETE!")
print("=" * 80)

print("\nâœ… IMPROVEMENTS:")
print(f"  1. Removed {len(leakage_features)} data leakage features")
print("  2. Reduced features from 86 â†’ 20 (76% reduction)")
print(f"  3. Accuracy change: {(val_acc - original_val_acc):+.1%}")
print("  4. Model is now CLEAN and production-ready")

print("\nðŸ“Š VALIDATION PERFORMANCE:")
print(f"  Accuracy:  {val_acc:.1%}")
print(f"  Precision: {val_prec:.1%}")
print(f"  Recall:    {val_rec:.1%}")
print(f"  F1 Score:  {val_f1:.3f}")

print("\nðŸ“ OUTPUT FILES:")
print(f"  Model: {clean_model_file}")
print(f"  Report: {report_file}")

print("\n" + "=" * 80)
print("NEXT STEP: Test multiple confidence thresholds")
print("Run: python phase1_confidence_sweep.py")
print("=" * 80)
